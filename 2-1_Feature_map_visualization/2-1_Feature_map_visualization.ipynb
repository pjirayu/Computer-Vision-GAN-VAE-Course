{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "example_3_5_Comparison_of_Latent_Vector.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0f5814520ed74b51865957291691bdd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c8d3072dcdfc4e5798226b3ef8f74367",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_add6b9675b62433396acaf4b614944b0",
              "IPY_MODEL_a442a84b9e954aea8cb64fe170c80e71"
            ]
          }
        },
        "c8d3072dcdfc4e5798226b3ef8f74367": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "add6b9675b62433396acaf4b614944b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_adaa30ade8e24caca12dbba982667768",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c51b2e746a214130a9fe26f64f2fd8f6"
          }
        },
        "a442a84b9e954aea8cb64fe170c80e71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4ff2ceba000c4202ba52bd81af4108e0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [00:10&lt;00:00, 55.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a140330d3131443b9f058e4a8cf24225"
          }
        },
        "adaa30ade8e24caca12dbba982667768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c51b2e746a214130a9fe26f64f2fd8f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4ff2ceba000c4202ba52bd81af4108e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a140330d3131443b9f058e4a8cf24225": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhRIufI0Xhn1"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch.nn as nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hYzFka9Xj87"
      },
      "source": [
        "def preprocess_image(cv2im, resize_im=True):\n",
        "\n",
        "    # Resize image\n",
        "    if resize_im:\n",
        "        cv2im = cv2.resize(cv2im, (224, 224))\n",
        "    im_as_arr = np.float32(cv2im)\n",
        "    im_as_arr = np.ascontiguousarray(im_as_arr[..., ::-1])\n",
        "    im_as_arr = im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n",
        "    # Normalize the channels\n",
        "    for channel, _ in enumerate(im_as_arr):\n",
        "        im_as_arr[channel] /= 255\n",
        "    # Convert to float tensor\n",
        "    im_as_ten = torch.from_numpy(im_as_arr).float()\n",
        "    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n",
        "    im_as_ten.unsqueeze_(0)\n",
        "    # Convert to Pytorch variable\n",
        "    im_as_var = Variable(im_as_ten, requires_grad=True)\n",
        "    return im_as_var"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkptXxluXmbL"
      },
      "source": [
        "class FeatureVisualization():\n",
        "    def __init__(self,img_path,selected_layer):\n",
        "        self.img_path=img_path\n",
        "        self.selected_layer=selected_layer\n",
        "        # Load pretrained model\n",
        "        \n",
        "        self.pretrained_model = models.vgg16(pretrained=True).features\n",
        "        self.pretrained_model.eval()\n",
        "        self.pretrained_model2 = models.vgg16(pretrained=True)\n",
        "        #self.pretrained_model2 = models.resnet18(pretrained=True)\n",
        "        self.pretrained_model2.eval()\n",
        "    def process_image(self):\n",
        "        img=cv2.imread(self.img_path)\n",
        "        img=preprocess_image(img)\n",
        "        return img\n",
        "\n",
        "    def get_feature(self):\n",
        "        # Image  preprocessing\n",
        "        input=self.process_image()\n",
        "        #print(\"input.shape:{}\".format(input.shape))\n",
        "        x=input\n",
        "        for index,layer in enumerate(self.pretrained_model):\n",
        "            x=layer(x)\n",
        "            #print(\"x:{}\".format(x.shape))\n",
        "            if (index == self.selected_layer):\n",
        "                return x\n",
        "\n",
        "    def get_single_feature(self):\n",
        "        # Get the feature map\n",
        "\n",
        "        features=self.get_feature()\n",
        "        #print(features.shape)\n",
        "        feature=features[:,0,:,:]\n",
        "        feature=feature.view(feature.shape[1],feature.shape[2])\n",
        "\n",
        "        #print(\"feature\")\n",
        "        #print(feature.shape)\n",
        "        return feature\n",
        "\n",
        "    def get_multi_feature(self):\n",
        "        # Get the feature map\n",
        "        features=self.get_feature()\n",
        "        #print(features.shape)\n",
        "        result_path = './feat_first' + str(self.selected_layer)\n",
        "\n",
        "        if not os.path.exists(result_path):\n",
        "            os.makedirs(result_path)\n",
        "        print(\"On layer:{}, We can get the {} feature maps\".format(self.selected_layer,features.shape[1]))    \n",
        "        #print(features.shape[1])\n",
        "        for i in range(features.shape[1]):\n",
        "            feature=features[:,i,:,:]\n",
        "            feature=feature.view(feature.shape[1],feature.shape[2])\n",
        "            feature = feature.data.numpy()\n",
        "            feature = 1.0 / (1 + np.exp(-1 * feature))\n",
        "            feature = np.round(feature * 255)\n",
        "            save_name = result_path + '/' + str(i) + '.jpg'\n",
        "            cv2.imwrite(save_name, feature)\n",
        "    def get_multi_feature1(self):\n",
        "        # Get the feature map\n",
        "        features=self.get_feature()\n",
        "        #print(features.shape)\n",
        "        result_path = './feat_second' + str(self.selected_layer)\n",
        "\n",
        "        if not os.path.exists(result_path):\n",
        "            os.makedirs(result_path)\n",
        "        print(\"On layer:{}, We can get the {} feature maps\".format(self.selected_layer,features.shape[1]))    \n",
        "        #print(features.shape[1])\n",
        "        for i in range(features.shape[1]):\n",
        "            feature=features[:,i,:,:]\n",
        "            feature=feature.view(feature.shape[1],feature.shape[2])\n",
        "            feature = feature.data.numpy()\n",
        "            feature = 1.0 / (1 + np.exp(-1 * feature))\n",
        "            feature = np.round(feature * 255)\n",
        "            save_name = result_path + '/' + str(i) + '.jpg'\n",
        "            cv2.imwrite(save_name, feature)\n",
        "\n",
        "    def save_feature_to_img(self):\n",
        "        #to numpy\n",
        "        feature=self.get_single_feature()\n",
        "        self.get_multi_feature()\n",
        "        feature=feature.data.numpy()\n",
        "\n",
        "        #use sigmod to [0,1]\n",
        "        # print(feature[0])\n",
        "        feature= 1.0/(1+np.exp(-1*feature))\n",
        "\n",
        "        # to [0,255]\n",
        "        feature=np.round(feature*255)\n",
        "        #print(self.selected_layer)\n",
        "        save_name = './feat_first' + str(self.selected_layer) + '.jpg'\n",
        "        cv2.imwrite(save_name, feature)\n",
        "    def save_feature_to_img1(self):\n",
        "        #to numpy\n",
        "        feature=self.get_single_feature()\n",
        "        self.get_multi_feature1()\n",
        "        feature=feature.data.numpy()\n",
        "\n",
        "        #use sigmod to [0,1]\n",
        "        # print(feature[0])\n",
        "        feature= 1.0/(1+np.exp(-1*feature))\n",
        "\n",
        "        # to [0,255]\n",
        "        feature=np.round(feature*255)\n",
        "        #print(self.selected_layer)\n",
        "        save_name = './feat_second' + str(self.selected_layer) + '.jpg'\n",
        "        cv2.imwrite(save_name, feature)\n",
        "    def plot_probablity(self,outputs):\n",
        "\n",
        "        outputs = outputs.data.numpy()\n",
        "        outputs = np.ndarray.tolist(outputs)\n",
        "\n",
        "        x = range(0, 1000)\n",
        "        plt.bar(x, outputs[0])\n",
        "        plt.xlabel(\"Class\")\n",
        "        plt.ylabel(\"Probablity\")\n",
        "        plt.title(\"Image classifier\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def predict(self):\n",
        "        input=self.process_image()\n",
        "        outputs = self.pretrained_model2(input)\n",
        "\n",
        "        s = torch.nn.Softmax(dim=1)\n",
        "        result = s(outputs)\n",
        "        self.plot_probablity(result)\n",
        "\n",
        "        prob, predicted = result.sort(1,descending=True)\n",
        "        prob = prob.data.numpy()\n",
        "\n",
        "        predicted = predicted.data.numpy()\n",
        "        \n",
        "        print(\"Probablity TOP-3:\\n\")\n",
        "        print(\"\")\n",
        "        for i in range(3):\n",
        "            \n",
        "            print(\"TOP_\"+str(i+1))\n",
        "            print(\"Probablity:{}\".format(prob[0][i]))\n",
        "            print(\"Predicted:{}\\n\".format(c[int(predicted[0][i])]))\n",
        "        return outputs"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGgajl_4XogN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0f5814520ed74b51865957291691bdd0",
            "c8d3072dcdfc4e5798226b3ef8f74367",
            "add6b9675b62433396acaf4b614944b0",
            "a442a84b9e954aea8cb64fe170c80e71",
            "adaa30ade8e24caca12dbba982667768",
            "c51b2e746a214130a9fe26f64f2fd8f6",
            "4ff2ceba000c4202ba52bd81af4108e0",
            "a140330d3131443b9f058e4a8cf24225"
          ]
        },
        "outputId": "2643feb9-0356-4e2b-e35f-5928907918f6"
      },
      "source": [
        "if __name__=='__main__':\n",
        "  # get class\n",
        "  c = {}\n",
        "  with open(\"imagenet1000_clsidx_to_labels.txt\") as f:\n",
        "    for line in f:\n",
        "      (key, val) = line.split(\":\")\n",
        "      c[int(key)] = val.split(\",\")[0]\n",
        "  # Define image path and select the layer\n",
        "  myClass=FeatureVisualization('./jellyfish.jpg',5)\n",
        "  print(myClass.pretrained_model2)\n",
        "\n",
        "  myClass.save_feature_to_img()\n",
        "  myClass.predict()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f5814520ed74b51865957291691bdd0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n",
            "On layer:2, We can get the 64 feature maps\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Probablity TOP-3:\n",
            "\n",
            "\n",
            "TOP_1\n",
            "Probablity:0.9990069270133972\n",
            "Predicted: 'jellyfish'\n",
            "\n",
            "TOP_2\n",
            "Probablity:0.0008054533391259611\n",
            "Predicted: 'isopod'\n",
            "\n",
            "TOP_3\n",
            "Probablity:8.906585571821779e-05\n",
            "Predicted: 'chambered nautilus\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}